{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "invalid-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "narrative-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook of the jungle book, by rudyard kipling\n",
      "\n",
      "this ebook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  you may copy it, give it away or\n",
      "re-use it under the terms of the project gutenberg license included\n",
      "with this ebook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "title: the jungle book\n",
      "\n",
      "author: rudyard kipling\n",
      "\n",
      "release date: january 16, 2006 [ebook #236]\n",
      "last updated: october 6, 2016\n",
      "\n",
      "language: english\n",
      "\n",
      "character set encoding: utf-8\n",
      "\n",
      "*** sta\n"
     ]
    }
   ],
   "source": [
    "#LOAD TEXT\n",
    "#Save notepad as UTF-8 (select from dropdown during saving)\n",
    "filename = \"data.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "print(raw_text[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fabulous-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN TEXT\n",
    "#Remove numbers\n",
    "raw_text = ''.join(c for c in raw_text if not c.isdigit())\n",
    "\n",
    "#How many total characters do we have in our training text?\n",
    "chars = sorted(list(set(raw_text))) #List of every character\n",
    "\n",
    "#Character sequences must be encoded as integers. \n",
    "#Each unique character will be assigned an integer value. \n",
    "#Create a dictionary of characters mapped to integer values\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "#Do the reverse so we can print our predictions in characters and not integers\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "superb-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters in the text; corpus length:  292868\n",
      "Total Vocab:  50\n"
     ]
    }
   ],
   "source": [
    "# summarize the data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters in the text; corpus length: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "thousand-paragraph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 29281\n"
     ]
    }
   ],
   "source": [
    "seq_length = 60  #Length of each input sequence\n",
    "step = 10   #Instead of moving 1 letter at a time, try skipping a few. \n",
    "sentences = []    # X values (Sentences)\n",
    "next_chars = []   # Y values. The character that follows the sentence defined as X\n",
    "for i in range(0, n_chars - seq_length, step):  #step=1 means each sentence is offset just by a single letter\n",
    "    sentences.append(raw_text[i: i + seq_length])  #Sequence in\n",
    "    next_chars.append(raw_text[i + seq_length])  #Sequence out\n",
    "n_patterns = len(sentences)    \n",
    "print('Number of sequences:', n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "exterior-headset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29281, 60, 50)\n",
      "(29281, 50)\n",
      "[[False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False  True False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False  True False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False  True False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False  True False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False  True False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False  True False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False  True False False False False False False False False\n",
      "  False False]]\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[next_chars[i]]] = 1\n",
    "    \n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dimensional-destiny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6450      \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "shaped-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"saved_weights/saved_weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "developmental-iceland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 2.5115\n",
      "Epoch 00001: loss improved from inf to 2.51146, saving model to saved_weights\\saved_weights-01-2.5115.hdf5\n",
      "229/229 [==============================] - 17s 76ms/step - loss: 2.5115\n",
      "Epoch 2/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 2.0282\n",
      "Epoch 00002: loss improved from 2.51146 to 2.02823, saving model to saved_weights\\saved_weights-02-2.0282.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 2.0282\n",
      "Epoch 3/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.8327\n",
      "Epoch 00003: loss improved from 2.02823 to 1.83267, saving model to saved_weights\\saved_weights-03-1.8327.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 1.8327\n",
      "Epoch 4/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.6878\n",
      "Epoch 00004: loss improved from 1.83267 to 1.68779, saving model to saved_weights\\saved_weights-04-1.6878.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 1.6878\n",
      "Epoch 5/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.5673\n",
      "Epoch 00005: loss improved from 1.68779 to 1.56733, saving model to saved_weights\\saved_weights-05-1.5673.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 1.5673\n",
      "Epoch 6/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.4596\n",
      "Epoch 00006: loss improved from 1.56733 to 1.45957, saving model to saved_weights\\saved_weights-06-1.4596.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 1.4596\n",
      "Epoch 7/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.3678\n",
      "Epoch 00007: loss improved from 1.45957 to 1.36782, saving model to saved_weights\\saved_weights-07-1.3678.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 1.3678\n",
      "Epoch 8/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.2909\n",
      "Epoch 00008: loss improved from 1.36782 to 1.29093, saving model to saved_weights\\saved_weights-08-1.2909.hdf5\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 1.2909\n",
      "Epoch 9/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.2222\n",
      "Epoch 00009: loss improved from 1.29093 to 1.22215, saving model to saved_weights\\saved_weights-09-1.2222.hdf5\n",
      "229/229 [==============================] - 18s 80ms/step - loss: 1.2222\n",
      "Epoch 10/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.1710\n",
      "Epoch 00010: loss improved from 1.22215 to 1.17098, saving model to saved_weights\\saved_weights-10-1.1710.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 1.1710\n",
      "Epoch 11/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.1306\n",
      "Epoch 00011: loss improved from 1.17098 to 1.13063, saving model to saved_weights\\saved_weights-11-1.1306.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 1.1306\n",
      "Epoch 12/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.0882\n",
      "Epoch 00012: loss improved from 1.13063 to 1.08825, saving model to saved_weights\\saved_weights-12-1.0882.hdf5\n",
      "229/229 [==============================] - 19s 81ms/step - loss: 1.0882\n",
      "Epoch 13/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.0658\n",
      "Epoch 00013: loss improved from 1.08825 to 1.06581, saving model to saved_weights\\saved_weights-13-1.0658.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 1.0658\n",
      "Epoch 14/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.0403\n",
      "Epoch 00014: loss improved from 1.06581 to 1.04033, saving model to saved_weights\\saved_weights-14-1.0403.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 1.0403\n",
      "Epoch 15/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.0191\n",
      "Epoch 00015: loss improved from 1.04033 to 1.01911, saving model to saved_weights\\saved_weights-15-1.0191.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 1.0191\n",
      "Epoch 16/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 1.0026\n",
      "Epoch 00016: loss improved from 1.01911 to 1.00261, saving model to saved_weights\\saved_weights-16-1.0026.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 1.0026\n",
      "Epoch 17/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9872\n",
      "Epoch 00017: loss improved from 1.00261 to 0.98723, saving model to saved_weights\\saved_weights-17-0.9872.hdf5\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 0.9872\n",
      "Epoch 18/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9744\n",
      "Epoch 00018: loss improved from 0.98723 to 0.97436, saving model to saved_weights\\saved_weights-18-0.9744.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 0.9744\n",
      "Epoch 19/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9329\n",
      "Epoch 00019: loss improved from 0.97436 to 0.93293, saving model to saved_weights\\saved_weights-19-0.9329.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 0.9329\n",
      "Epoch 20/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9389\n",
      "Epoch 00020: loss did not improve from 0.93293\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 0.9389\n",
      "Epoch 21/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9215\n",
      "Epoch 00021: loss improved from 0.93293 to 0.92149, saving model to saved_weights\\saved_weights-21-0.9215.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 0.9215\n",
      "Epoch 22/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9214\n",
      "Epoch 00022: loss improved from 0.92149 to 0.92140, saving model to saved_weights\\saved_weights-22-0.9214.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 0.9214\n",
      "Epoch 23/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.9045\n",
      "Epoch 00023: loss improved from 0.92140 to 0.90453, saving model to saved_weights\\saved_weights-23-0.9045.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 0.9045\n",
      "Epoch 24/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8915\n",
      "Epoch 00024: loss improved from 0.90453 to 0.89149, saving model to saved_weights\\saved_weights-24-0.8915.hdf5\n",
      "229/229 [==============================] - 19s 81ms/step - loss: 0.8915\n",
      "Epoch 25/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8780\n",
      "Epoch 00025: loss improved from 0.89149 to 0.87798, saving model to saved_weights\\saved_weights-25-0.8780.hdf5\n",
      "229/229 [==============================] - 22s 98ms/step - loss: 0.8780\n",
      "Epoch 26/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8634\n",
      "Epoch 00026: loss improved from 0.87798 to 0.86344, saving model to saved_weights\\saved_weights-26-0.8634.hdf5\n",
      "229/229 [==============================] - 20s 86ms/step - loss: 0.8634\n",
      "Epoch 27/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8482\n",
      "Epoch 00027: loss improved from 0.86344 to 0.84823, saving model to saved_weights\\saved_weights-27-0.8482.hdf5\n",
      "229/229 [==============================] - 19s 81ms/step - loss: 0.8482\n",
      "Epoch 28/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8425\n",
      "Epoch 00028: loss improved from 0.84823 to 0.84248, saving model to saved_weights\\saved_weights-28-0.8425.hdf5\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 0.8425\n",
      "Epoch 29/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8305\n",
      "Epoch 00029: loss improved from 0.84248 to 0.83051, saving model to saved_weights\\saved_weights-29-0.8305.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 0.8305\n",
      "Epoch 30/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8175\n",
      "Epoch 00030: loss improved from 0.83051 to 0.81752, saving model to saved_weights\\saved_weights-30-0.8175.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 0.8175\n",
      "Epoch 31/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.8049\n",
      "Epoch 00031: loss improved from 0.81752 to 0.80486, saving model to saved_weights\\saved_weights-31-0.8049.hdf5\n",
      "229/229 [==============================] - 17s 76ms/step - loss: 0.8049\n",
      "Epoch 32/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7925\n",
      "Epoch 00032: loss improved from 0.80486 to 0.79253, saving model to saved_weights\\saved_weights-32-0.7925.hdf5\n",
      "229/229 [==============================] - 17s 76ms/step - loss: 0.7925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7902\n",
      "Epoch 00033: loss improved from 0.79253 to 0.79018, saving model to saved_weights\\saved_weights-33-0.7902.hdf5\n",
      "229/229 [==============================] - 19s 82ms/step - loss: 0.7902\n",
      "Epoch 34/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7758\n",
      "Epoch 00034: loss improved from 0.79018 to 0.77581, saving model to saved_weights\\saved_weights-34-0.7758.hdf5\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 0.7758\n",
      "Epoch 35/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7598\n",
      "Epoch 00035: loss improved from 0.77581 to 0.75976, saving model to saved_weights\\saved_weights-35-0.7598.hdf5\n",
      "229/229 [==============================] - 19s 81ms/step - loss: 0.7598\n",
      "Epoch 36/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7468\n",
      "Epoch 00036: loss improved from 0.75976 to 0.74681, saving model to saved_weights\\saved_weights-36-0.7468.hdf5\n",
      "229/229 [==============================] - 19s 82ms/step - loss: 0.7468\n",
      "Epoch 37/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7361\n",
      "Epoch 00037: loss improved from 0.74681 to 0.73606, saving model to saved_weights\\saved_weights-37-0.7361.hdf5\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 0.7361\n",
      "Epoch 38/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7424\n",
      "Epoch 00038: loss did not improve from 0.73606\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 0.7424\n",
      "Epoch 39/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7372\n",
      "Epoch 00039: loss did not improve from 0.73606\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 0.7372\n",
      "Epoch 40/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7180\n",
      "Epoch 00040: loss improved from 0.73606 to 0.71804, saving model to saved_weights\\saved_weights-40-0.7180.hdf5\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 0.7180\n",
      "Epoch 41/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.7096\n",
      "Epoch 00041: loss improved from 0.71804 to 0.70964, saving model to saved_weights\\saved_weights-41-0.7096.hdf5\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 0.7096\n",
      "Epoch 42/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6974\n",
      "Epoch 00042: loss improved from 0.70964 to 0.69740, saving model to saved_weights\\saved_weights-42-0.6974.hdf5\n",
      "229/229 [==============================] - 19s 83ms/step - loss: 0.6974\n",
      "Epoch 43/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6896\n",
      "Epoch 00043: loss improved from 0.69740 to 0.68959, saving model to saved_weights\\saved_weights-43-0.6896.hdf5\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 0.6896\n",
      "Epoch 44/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6790\n",
      "Epoch 00044: loss improved from 0.68959 to 0.67899, saving model to saved_weights\\saved_weights-44-0.6790.hdf5\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 0.6790\n",
      "Epoch 45/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6711\n",
      "Epoch 00045: loss improved from 0.67899 to 0.67115, saving model to saved_weights\\saved_weights-45-0.6711.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 0.6711\n",
      "Epoch 46/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6634\n",
      "Epoch 00046: loss improved from 0.67115 to 0.66343, saving model to saved_weights\\saved_weights-46-0.6634.hdf5\n",
      "229/229 [==============================] - 18s 78ms/step - loss: 0.6634\n",
      "Epoch 47/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6568\n",
      "Epoch 00047: loss improved from 0.66343 to 0.65683, saving model to saved_weights\\saved_weights-47-0.6568.hdf5\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 0.6568\n",
      "Epoch 48/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6514\n",
      "Epoch 00048: loss improved from 0.65683 to 0.65138, saving model to saved_weights\\saved_weights-48-0.6514.hdf5\n",
      "229/229 [==============================] - 18s 81ms/step - loss: 0.6514\n",
      "Epoch 49/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6427\n",
      "Epoch 00049: loss improved from 0.65138 to 0.64273, saving model to saved_weights\\saved_weights-49-0.6427.hdf5\n",
      "229/229 [==============================] - 18s 80ms/step - loss: 0.6427\n",
      "Epoch 50/50\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6411\n",
      "Epoch 00050: loss improved from 0.64273 to 0.64114, saving model to saved_weights\\saved_weights-50-0.6411.hdf5\n",
      "229/229 [==============================] - 21s 90ms/step - loss: 0.6411\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=50,   \n",
    "          callbacks=callbacks_list)\n",
    "\n",
    "model.save('my_saved_weights_jungle_book_50epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "green-recruitment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "strong-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds) #exp of log (x), isn't this same as x??\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "interim-potato",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Seed for our text prediction: \"t\n",
      "of a hook-and-eye fastening. he was afraid for the minute,\"\n"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "# load the network weights\n",
    "filename = \"my_saved_weights_jungle_book_50epochs.h5\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "#Pick a random sentence from the text as seed.\n",
    "start_index = random.randint(0, n_chars - seq_length - 1)\n",
    "\n",
    "#Initiate generated text and keep adding new predictions and print them out\n",
    "generated = ''\n",
    "sentence = raw_text[start_index: start_index + seq_length]\n",
    "generated += sentence\n",
    "\n",
    "print('----- Seed for our text prediction: \"' + sentence + '\"')\n",
    "#sys.stdout.write(generated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "after-uniform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "” said the jungle. sham the sea look\n",
      "helves would cin to-titi-tle find the\n",
      "lust grop, and we cungaraled miger-tundra. this was their dy. sheather.  what because he\n",
      "naughe. then you canyighling. the man’y elephants to\n",
      "shill as sirven the dark; and\n",
      "the firnding hand o” mongars, should sides.”\n",
      "\n",
      "rikitioks weather, darze.” bugat--was siacing up where khan he man under answays into don’t foot?” bring br\n"
     ]
    }
   ],
   "source": [
    "for i in range(400):   # Number of characters including spaces\n",
    "    x_pred = np.zeros((1, seq_length, n_vocab))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = int_to_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "formal-religious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp37-cp37m-win_amd64.whl (8.5 MB)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\19055\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\19055\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\19055\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six in c:\\users\\19055\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp37-cp37m-win_amd64.whl (51 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.1.0-cp37-cp37m-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: pillow, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.4 pillow-8.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-walker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
